{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb22418",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cb22418",
    "outputId": "a2ecad18-284f-48c2-c472-780f4f5c223d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 17:09:57.455456: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-05 17:09:57.455477: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thuy/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pickle\n",
    "import tenseal as ts\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1_5vtWIZUQhB",
   "metadata": {
    "id": "1_5vtWIZUQhB"
   },
   "source": [
    "Read the data and check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fLL4_HYOWz9x",
   "metadata": {
    "id": "fLL4_HYOWz9x"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "# target = 1 - data.target\n",
    "\n",
    "# MUST reshape target (y) to be list of lists for TensorFlow \n",
    "target = data.target.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "SKmSZhgggMqu",
   "metadata": {
    "id": "SKmSZhgggMqu"
   },
   "outputs": [],
   "source": [
    "temp = np.zeros(target.size)\n",
    "target = np.insert(target, 0, temp, axis=1)\n",
    "for i in range(len(target)):\n",
    "  target[i][0] = 1 - target[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "I_Hb4D5US0Uw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_Hb4D5US0Uw",
    "outputId": "418689c2-b81d-4d87-c317-99c77a1c17bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(data.target.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2SZ-hfundNCX",
   "metadata": {
    "id": "2SZ-hfundNCX"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Bt6JhPOGdVEx",
   "metadata": {
    "id": "Bt6JhPOGdVEx"
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Bdhn77kBtSC_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bdhn77kBtSC_",
    "outputId": "1cb1a43c-c640-4141-eceb-758a23fb6a1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30650268,  0.62104272, -0.16828882, -0.36588129,  2.19035687,\n",
       "         1.77664083,  1.27444525,  1.17841776,  2.02983185,  1.61676872,\n",
       "        -0.33685784, -0.40786249, -0.20881921, -0.32773331, -0.44966412,\n",
       "         0.59461344,  0.14631533,  0.10335768,  0.13164223,  0.02065071,\n",
       "        -0.13857129,  0.8430959 , -0.00521991, -0.22513212,  1.66926677,\n",
       "         1.83915393,  1.33954402,  1.44311687,  2.37234416,  1.29276721],\n",
       "       [ 1.29198801, -0.46466634,  1.22185119,  1.18250508,  0.65188085,\n",
       "         0.15325043,  0.7614691 ,  1.00020322,  0.6074009 , -0.06209409,\n",
       "         1.06800303,  0.63523719,  0.98549316,  1.02784828,  0.36244732,\n",
       "        -0.22544912,  0.07739708,  0.74161963,  0.0042158 ,  0.34936176,\n",
       "         1.27241411, -0.09340583,  1.16718532,  1.19251002,  0.80945305,\n",
       "        -0.12025427,  0.31168638,  0.84815064,  0.02745907,  0.07219191],\n",
       "       [-0.85439525, -0.99366076, -0.87320053, -0.75936928, -1.31441794,\n",
       "        -1.06273142, -0.75168296, -1.05662044, -0.98912967, -0.06066649,\n",
       "        -0.75434171, -0.97337024, -0.75727201, -0.56011873, -0.9538514 ,\n",
       "        -0.59538455, -0.38053365, -1.2739265 , -1.10621452, -0.0642296 ,\n",
       "        -0.72509037, -0.82429483, -0.76518973, -0.649882  , -0.83709021,\n",
       "        -0.3798286 , -0.26727215, -0.87571007, -0.45289475,  0.45065651]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fJ1QZ4Vdg_C",
   "metadata": {
    "id": "3fJ1QZ4Vdg_C"
   },
   "outputs": [],
   "source": [
    "# shape None == allow dynamic number of rows; X_train.shape[1] == number of features\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, X_train.shape[1]), name='inputs')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(None, 2), name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eVytvr9pdtCK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVytvr9pdtCK",
    "outputId": "4f528083-a099-4c83-80ac-db2ef70b2424"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5705/1031069605.py:3: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  h1 = tf.layers.dense(X, 30, name='dense1')\n",
      "/home/thuy/.local/lib/python3.8/site-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n",
      "/tmp/ipykernel_5705/1031069605.py:7: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  h2 = tf.layers.dense(h1, 16, name='dense2')\n",
      "/tmp/ipykernel_5705/1031069605.py:10: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  model1 = tf.layers.dense(h2, 2, name='dense3')\n"
     ]
    }
   ],
   "source": [
    "# First hidden layer\n",
    "# X == features coming in, 30 == number of features == neurons\n",
    "h1 = tf.layers.dense(X, 30, name='dense1')\n",
    "h1 = h1 * h1\n",
    "# Second hidden layer\n",
    "# h1 == features coming in, 26 == number of neurons == arbitrary\n",
    "h2 = tf.layers.dense(h1, 16, name='dense2')\n",
    "h2 = h2 * h2\n",
    "# Last/output layar always has 1 neuron for binary classification problems\n",
    "model1 = tf.layers.dense(h2, 2, name='dense3')\n",
    "\n",
    "model = tf.sigmoid(model1, name='sigmoid')\n",
    "# model = approx_sigmoid(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WxeA8h4XgZuc",
   "metadata": {
    "id": "WxeA8h4XgZuc"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.log_loss(y, model)\n",
    "\n",
    "# Define training operation\n",
    "training_op = tf.train.AdamOptimizer(.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2Kzkv3CAgdId",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Kzkv3CAgdId",
    "outputId": "a47ec9e4-9866-485d-ec02-dc204fae0fd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-05 17:10:18.830889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-05 17:10:18.830917: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-05 17:10:18.830933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (thuy-ThinkPad-T480): /proc/driver/nvidia/version does not exist\n",
      "2022-03-05 17:10:18.831117: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  | training loss: 1.2439755  | test loss: 0.7964383\n",
      "epoch: 100  | training loss: 0.0072227735  | test loss: 0.18834677\n",
      "epoch: 200  | training loss: 0.0005067854  | test loss: 0.15602176\n",
      "epoch: 300  | training loss: 0.0002024181  | test loss: 0.113301605\n",
      "epoch: 400  | training loss: 0.00010763543  | test loss: 0.12240543\n",
      "epoch: 500  | training loss: 6.634898e-05  | test loss: 0.1292745\n",
      "epoch: 600  | training loss: 4.4768163e-05  | test loss: 0.13479961\n",
      "epoch: 700  | training loss: 3.210395e-05  | test loss: 0.13940835\n",
      "epoch: 800  | training loss: 2.405548e-05  | test loss: 0.14337963\n",
      "epoch: 900  | training loss: 1.86288e-05  | test loss: 0.14687671\n",
      "Accuracy: 0.97902095\n",
      "Accuracy without sigmoid:  0.97902095\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    # Train model with 1000 epochs\n",
    "    # saver.restore(sess, '/content/drive/MyDrive/Colab Notebooks/result/nn_data/net-1000')\n",
    "    for epoch in range(1000):\n",
    "        sess.run(training_op, feed_dict={X: X_train, y: y_train})\n",
    "        if (epoch % 100 == 0):\n",
    "            training_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "            test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "            print('epoch:',epoch,' | training loss:', training_loss, ' | test loss:', test_loss)\n",
    "#     saved_path = saver.save(sess, '/content/drive/MyDrive/Colab Notebooks/result/nn_data/net', global_step=1000)\n",
    "#     pickle.dump(sess, \"saver\")\n",
    "    \n",
    "    # # Use model to classify test data\n",
    "    # classifications_on_test_data = sess.run(model, feed_dict={X: X_test})\n",
    "    \n",
    "    # # Check model accuracy\n",
    "    # classes = (classifications_on_test_data > .5).astype(int)\n",
    "    \n",
    "    # print('\\nAccuracy Score:',metrics.accuracy_score(y_test, classes))\n",
    "    \n",
    "    predictions = tf.equal(tf.argmax(model, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: X_test, y: y_test}))\n",
    "    no_sig_predictions = tf.equal(tf.argmax(model1, 1), tf.argmax(y, 1))\n",
    "    no_sig_accuracy = tf.reduce_mean(tf.cast(no_sig_predictions, \"float\"))\n",
    "    print('Accuracy without sigmoid: ', no_sig_accuracy.eval({X: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeaf9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train plain model\n",
    "class train():\n",
    "    def __init__(self):\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, './nn_data/net-1000')\n",
    "            with tf.variable_scope('dense1', reuse=True):\n",
    "                self.dense1_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense1_bias = sess.run(tf.get_variable('bias'))\n",
    "            with tf.variable_scope('dense2', reuse=True):\n",
    "                self.dense2_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense2_bias = sess.run(tf.get_variable('bias'))\n",
    "            with tf.variable_scope('dense3', reuse=True):\n",
    "                self.dense3_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense3_bias = sess.run(tf.get_variable('bias'))\n",
    "            self.encoded_input = X.eval({X: X_test})\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #dense1\n",
    "        out = x.dot(self.dense1_kernel) + self.dense1_bias        \n",
    "        #square\n",
    "        out = out*out        \n",
    "        #dense2\n",
    "        out = out.dot(self.dense2_kernel) + self.dense2_bias\n",
    "        #square\n",
    "        out = out*out        \n",
    "        #dense3\n",
    "        out = out.dot(self.dense3_kernel) + self.dense3_bias\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0763d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train encrypted input\n",
    "class train_Enc_Model():\n",
    "    def __init__(self):\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, './nn_data/net-1000')\n",
    "            with tf.variable_scope('dense1', reuse=True):\n",
    "                self.dense1_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense1_bias = sess.run(tf.get_variable('bias'))\n",
    "            with tf.variable_scope('dense2', reuse=True):\n",
    "                self.dense2_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense2_bias = sess.run(tf.get_variable('bias'))\n",
    "            with tf.variable_scope('dense3', reuse=True):\n",
    "                self.dense3_kernel = sess.run(tf.get_variable('kernel'))\n",
    "                self.dense3_bias = sess.run(tf.get_variable('bias'))\n",
    "            self.encoded_input = X.eval({X: X_test})\n",
    "    \n",
    "    def forward(self, enc_x: ts.CKKSVector) -> ts.CKKSVector:\n",
    "        #dense1\n",
    "        out = enc_x.mm_(self.dense1_kernel) + self.dense1_bias        \n",
    "        #square\n",
    "        out = out.square_()        \n",
    "        #dense2\n",
    "        out = out.mm_(self.dense2_kernel) + self.dense2_bias        \n",
    "        #square\n",
    "        out = out.square_()        \n",
    "        #dense3\n",
    "        out = out.mm_(self.dense3_kernel) + self.dense3_bias\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6380210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encryption parameters\n",
    "poly_mod_degree = 2 ** 14\n",
    "power = 30\n",
    "coeff_mod_bit_sizes = [40] + ([power] * 6) + [40]\n",
    "\n",
    "# create TenSEALContext\n",
    "ctx_eval = ts.Context(ts.SCHEME_TYPE.CKKS,poly_mod_degree,-1,coeff_mod_bit_sizes=coeff_mod_bit_sizes,)\n",
    "\n",
    "# scale of ciphertext to use\n",
    "ctx_eval.global_scale = 2 ** power\n",
    "\n",
    "# this key is needed for doing dot-product operations\n",
    "ctx_eval.generate_galois_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92957f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./nn_data/net-1000\n",
      "INFO:tensorflow:Restoring parameters from ./nn_data/net-1000\n"
     ]
    }
   ],
   "source": [
    "enc_predictions = []\n",
    "plain_predictions = []\n",
    "\n",
    "ckks_model = train_Enc_Model()\n",
    "model = train()\n",
    "\n",
    "for index in range(len(X_test)):\n",
    "    #plain\n",
    "    result= model.forward(X_test[index])\n",
    "    result = np.argmax(result)\n",
    "    plain_predictions.append(result)\n",
    "    \n",
    "    #ckks\n",
    "    enc_sample = ts.ckks_vector(ctx_eval, X_test[index])\n",
    "    enc_prediction = ckks_model.forward(enc_sample)\n",
    "    dec_result = enc_prediction.decrypt(secret_key=ctx_eval.secret_key())\n",
    "    \n",
    "    dec_result = np.argmax(dec_result)\n",
    "    enc_predictions.append(dec_result)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "047d6c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plain_predictions[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "992d15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain model:================\n",
      "Different plain:  1\n",
      "Accuracy: plain 0.993006993006993\n",
      "Encrypted model:==================\n",
      "Different ckks:  3\n",
      "Accuracy ckks:  0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "enc_count = 0\n",
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if enc_predictions[i] != np.argmax(y_test[i]):\n",
    "        enc_count += 1\n",
    "    if plain_predictions[i] != np.argmax(y_test[i]):\n",
    "        count += 1\n",
    "        \n",
    "print(\"Plain model:================\")\n",
    "print(\"Different plain: \", count)\n",
    "print(\"Accuracy: plain\", 1 - count/len(y_test))\n",
    "\n",
    "print(\"Encrypted model:==================\")\n",
    "print(\"Different ckks: \", enc_count)\n",
    "print(\"Accuracy ckks: \", 1 - enc_count/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d54f2603",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./nn_data/net-1000\n",
      "Accuracy plain:  0.993007\n",
      "Accuracy CKKS:  0.97902095\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "plain_predict = tf.convert_to_tensor(plain_predictions, dtype=tf.int64)\n",
    "enc_predict = tf.convert_to_tensor(enc_predictions, dtype=tf.int64)\n",
    "# PRINT ACCURACIES\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './nn_data/net-1000')\n",
    "    \n",
    "    tf_guessed_predictions = tf.equal(plain_predict, tf.argmax(y, 1))\n",
    "    tf_accuracy = tf.reduce_mean(tf.cast(tf_guessed_predictions, tf.float32))\n",
    "\n",
    "    cn_guessed_predictions = tf.equal(enc_predict, tf.argmax(y, 1))\n",
    "    cn_accuracy = tf.reduce_mean(tf.cast(cn_guessed_predictions, tf.float32))\n",
    "\n",
    "    print(\"Accuracy plain: \", tf_accuracy.eval({X: X_test, y: y_test}))\n",
    "    print(\"Accuracy CKKS: \", cn_accuracy.eval({X: X_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41cadbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_predictions = []\n",
    "# train_tensor = train_Enc_tensor()\n",
    "\n",
    "# encrypted_tensor = ts.ckks_tensor(ctx_eval, X_test)\n",
    "# print(encrypted_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7319bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./nn_data/net-1000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './nn_data/net-1000')\n",
    "    with tf.variable_scope('dense1', reuse=True):\n",
    "        dense1_kernel = sess.run(tf.get_variable('kernel'))\n",
    "        dense1_bias = sess.run(tf.get_variable('bias'))\n",
    "    with tf.variable_scope('dense2', reuse=True):\n",
    "        dense2_kernel = sess.run(tf.get_variable('kernel'))\n",
    "        dense2_bias = sess.run(tf.get_variable('bias'))\n",
    "    with tf.variable_scope('dense3', reuse=True):\n",
    "        dense3_kernel = sess.run(tf.get_variable('kernel'))\n",
    "        dense3_bias = sess.run(tf.get_variable('bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d00202b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./ckks_data/dense1_kernel\", dense1_kernel)\n",
    "np.save(\"./ckks_data/dense1_bias\", dense1_kernel)\n",
    "np.save(\"./ckks_data/dense2_kernel\", dense2_kernel)\n",
    "np.save(\"./ckks_data/dense2_bias\", dense2_kernel)\n",
    "np.save(\"./ckks_data/dense3_kernel\", dense3_kernel)\n",
    "np.save(\"./ckks_data/dense3_bias\", dense3_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79c207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "general.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
